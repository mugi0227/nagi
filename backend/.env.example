# ===========================================
# nagi - Environment Variables
# ===========================================

# Environment: "local" or "gcp"
ENVIRONMENT=local

# Debug mode
DEBUG=true

# ===========================================
# Database
# ===========================================
# SQLite (local)
DATABASE_URL=sqlite+aiosqlite:///./secretary.db

# Firestore (gcp) - uses GOOGLE_APPLICATION_CREDENTIALS

# ===========================================
# LLM Configuration
# ===========================================
# LLM Provider: "gemini-api" (recommended), "vertex-ai" (GCP only), "litellm"
# - gemini-api: Gemini API (API Key, works in local/gcp)
# - vertex-ai: Vertex AI (GCP only, service account)
# - litellm: LiteLLM (Bedrock, OpenAI, etc. with optional custom endpoint)
LLM_PROVIDER=gemini-api

# Gemini model name (for gemini-api and vertex-ai)
GEMINI_MODEL=gemini-2.5-flash
# GEMINI_MODEL=gemini-3-pro-preview

# LiteLLM model identifier (for litellm provider)
LITELLM_MODEL=bedrock/anthropic.claude-3-5-sonnet-20241022-v2:0

# LiteLLM custom endpoint (optional, for proxy servers like LiteLLM Proxy)
LITELLM_API_BASE=

# LiteLLM custom API key (optional, for custom endpoints)
LITELLM_API_KEY=

# LiteLLM Vision model (optional, for better image understanding)
# If set, images will be processed by this model instead of the main model.
# Useful when main model (e.g., Claude) has weak vision, but you want qwen3-vl for images.
# Example: "openai/qwen3-vl" or leave empty to use main model
LITELLM_VISION_MODEL=

# Available models for model selection UI (comma-separated)
# For gemini-api/vertex-ai: list Gemini model names
# For litellm: leave empty to fetch from proxy server, or list model IDs
# Example: AVAILABLE_MODELS=gemini-2.5-flash,gemini-2.5-pro,gemini-3-flash-preview
AVAILABLE_MODELS=

# ===========================================
# Google Cloud (for GCP environment)
# ===========================================
# If empty, project ID is auto-detected from Application Default Credentials (ADC)
GOOGLE_CLOUD_PROJECT=
# ADC credentials:
# - Local: set to your service account JSON key path
# - GCE/Cloud Run: leave empty and use attached service account
GOOGLE_APPLICATION_CREDENTIALS=

# Gemini API Key (for gemini-api provider - get from https://aistudio.google.com/apikey)
GOOGLE_API_KEY=

# ===========================================
# AWS (for local Bedrock via LiteLLM)
# ===========================================
AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=
AWS_REGION=us-east-1

# ===========================================
# Firebase Auth (for GCP environment)
# ===========================================
FIREBASE_PROJECT_ID=your-firebase-project

# ===========================================
# Auth (OIDC/JWT)
# ===========================================
# mock | oidc | local
AUTH_PROVIDER=mock
OIDC_ISSUER=https://your-issuer-domain/
OIDC_AUDIENCE=your-client-id
# If omitted, defaults to {OIDC_ISSUER}/.well-known/jwks.json
OIDC_JWKS_URL=
OIDC_EMAIL_CLAIM=email
OIDC_NAME_CLAIM=name
OIDC_ALLOW_EMAIL_LINKING=false

# ===========================================
# Local Auth (password + JWT)
# ===========================================
# Only required when AUTH_PROVIDER=local
LOCAL_JWT_SECRET=replace-with-a-long-random-string
LOCAL_JWT_ISSUER=secretary-local
LOCAL_JWT_EXPIRE_MINUTES=10080

# ===========================================
# Registration Whitelist
# ===========================================
# Comma-separated list of allowed email addresses for registration.
# If empty, all emails are allowed (no restriction).
# Example: user1@example.com,user2@example.com
REGISTRATION_WHITELIST_EMAILS=

# ===========================================
# Developer Accounts
# ===========================================
# Comma-separated list of email addresses that are developer accounts.
# Developer accounts can change issue (feature request) statuses.
# Example: dev@example.com,admin@example.com
DEVELOPER_EMAILS=

# ===========================================
# Server
# ===========================================
HOST=0.0.0.0
PORT=8080

# CORS
ALLOWED_ORIGINS=["http://localhost:3000","http://localhost:5173"]

# ===========================================
# Scheduler
# ===========================================
# Quiet hours (no notifications)
QUIET_HOURS_START=02:00
QUIET_HOURS_END=06:00

# ===========================================
# Speech-to-Text
# ===========================================
# Speech provider: "whisper" | "google-stt-v2" | "amazon-transcribe"
SPEECH_PROVIDER=whisper

# Whisper model size for local development
WHISPER_MODEL_SIZE=base

# Google Cloud Speech-to-Text v2 (Chirp 3)
# location example: us / eu / global
STT_V2_LOCATION=us
STT_V2_MODEL=chirp_3
STT_V2_LANGUAGE=ja-JP

# Amazon Transcribe
# Requires AWS credentials with s3 + transcribe permissions
# and an existing S3 bucket for temporary audio upload.
AWS_TRANSCRIBE_S3_BUCKET=
AWS_TRANSCRIBE_S3_PREFIX=transcribe-input
AWS_TRANSCRIBE_LANGUAGE=ja-JP
AWS_TRANSCRIBE_POLL_SECONDS=0.35
AWS_TRANSCRIBE_TIMEOUT_SECONDS=180

# ===========================================
# Feature Flags
# ===========================================
# Enable the Issues (要望) page - GitHub Issues-like feedback system
# Set to true for internal/company use, false for public SaaS
ENABLE_ISSUES=false

# ===========================================
# Input Validation Limits
# ===========================================
# Maximum character lengths for input validation
MAX_TEXT_LENGTH=10000
MAX_TRANSCRIPTION_LENGTH=10000
MAX_IMAGE_ANALYSIS_LENGTH=5000
